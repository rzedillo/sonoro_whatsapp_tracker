# Deployment Scripts and Setup Guides

> üõ†Ô∏è **Automated Setup**: Complete collection of scripts for seamless local development and Google Cloud deployment.

## Navigation
- **Previous**: [CI/CD Pipeline](cicd_pipeline.md)
- **Next**: [Framework Integration](framework_integration.md)
- **Related**: [Local Development](local_development.md) ‚Üí [Google Cloud Setup](google_cloud_setup.md)

---

## Overview

This collection provides automation scripts for the complete development and deployment lifecycle, from initial project setup to production deployment and monitoring.

## Quick Start Scripts

### Complete Project Setup
```bash
#!/bin/bash
# scripts/setup-project.sh - Complete project initialization

set -e

PROJECT_NAME=${1}
OPENAI_API_KEY=${2}

if [ -z "$PROJECT_NAME" ] || [ -z "$OPENAI_API_KEY" ]; then
    echo "‚ùå Usage: $0 <project-name> <openai-api-key>"
    echo "   Example: $0 my-ai-agents sk-..."
    exit 1
fi

echo "üöÄ Setting up AI Agents project: $PROJECT_NAME"

# Validate project name
if [[ ! "$PROJECT_NAME" =~ ^[a-z][a-z0-9-]*[a-z0-9]$ ]]; then
    echo "‚ùå Project name must be lowercase, start with letter, contain only letters, numbers, and hyphens"
    exit 1
fi

# Create project directory structure
echo "üìÅ Creating project structure..."
mkdir -p $PROJECT_NAME/{backend,frontend,database,infrastructure,tests,docs,deployment}
cd $PROJECT_NAME

# Create subdirectories
mkdir -p backend/{core/{agents,orchestrator,models,utils},api/v1,config}
mkdir -p frontend/{pages,components,styles,static}
mkdir -p database/{migrations,schemas,seeds,scripts}
mkdir -p infrastructure/{terraform/{environments/{dev,staging,prod},modules},docker}
mkdir -p tests/{unit,integration,e2e,performance,fixtures}
mkdir -p docs/{api,deployment,development}
mkdir -p deployment/{scripts,configs,monitoring}

# Create environment configuration
echo "‚öôÔ∏è Creating environment configuration..."
cat > .env.template << EOF
# ${PROJECT_NAME^^}_ENV Environment Configuration
# Copy this file to .env.local and update with your actual values

# Environment Settings
${PROJECT_NAME^^}_ENV=development
${PROJECT_NAME^^}_DEBUG=true
${PROJECT_NAME^^}_LOG_LEVEL=INFO
${PROJECT_NAME^^}_SECRET_KEY=change-this-in-production

# Database Configuration
${PROJECT_NAME^^}_DATABASE_URL=postgresql://localhost:5432/${PROJECT_NAME}_dev
${PROJECT_NAME^^}_DATABASE_HOST=localhost
${PROJECT_NAME^^}_DATABASE_PORT=5432
${PROJECT_NAME^^}_DATABASE_NAME=${PROJECT_NAME}_dev
${PROJECT_NAME^^}_DATABASE_USER=${PROJECT_NAME}_user
${PROJECT_NAME^^}_DATABASE_PASSWORD=dev_password

# LLM API Keys
${PROJECT_NAME^^}_OPENAI_API_KEY=your-openai-api-key-here
${PROJECT_NAME^^}_OPENAI_MODEL=gpt-4o-mini
${PROJECT_NAME^^}_OPENAI_TIMEOUT=30
${PROJECT_NAME^^}_ANTHROPIC_API_KEY=your-claude-api-key-here

# Web Interface
${PROJECT_NAME^^}_FRONTEND_HOST=localhost
${PROJECT_NAME^^}_FRONTEND_PORT=8501
${PROJECT_NAME^^}_BACKEND_HOST=localhost
${PROJECT_NAME^^}_BACKEND_PORT=8000
${PROJECT_NAME^^}_CORS_ORIGINS=http://localhost:8501

# Google Cloud (for production deployment)
${PROJECT_NAME^^}_GCP_PROJECT_ID=your-gcp-project-id
${PROJECT_NAME^^}_GCP_REGION=us-central1
${PROJECT_NAME^^}_GOOGLE_APPLICATION_CREDENTIALS=./credentials/service-account.json

# Redis/Caching
${PROJECT_NAME^^}_REDIS_URL=redis://localhost:6379/0
${PROJECT_NAME^^}_CACHE_TTL=3600

# Security
${PROJECT_NAME^^}_JWT_SECRET=your-jwt-secret-here
${PROJECT_NAME^^}_ENCRYPTION_KEY=your-32-character-encryption-key
${PROJECT_NAME^^}_RATE_LIMIT_REQUESTS=100
${PROJECT_NAME^^}_RATE_LIMIT_WINDOW=3600
EOF

# Create .env.local with provided API key
cp .env.template .env.local
sed -i.bak "s/your-openai-api-key-here/$OPENAI_API_KEY/" .env.local
rm .env.local.bak

# Create .gitignore
echo "üõ°Ô∏è Creating .gitignore..."
cat > .gitignore << 'EOF'
# Environment files - NEVER commit these
.env
.env.local
.env.development
.env.staging
.env.production
*.env

# API Keys and Secrets
*key*
*secret*
*token*
*credential*
*password*
credentials/
secrets/

# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# Virtual environments
env/
venv/
ENV/
env.bak/
venv.bak/
.venv/

# IDE
.vscode/
.idea/
*.swp
*.swo

# OS
.DS_Store
Thumbs.db

# Logs
*.log
logs/

# Database
*.db
*.sqlite

# Docker
.dockerignore

# Terraform
*.tfstate
*.tfstate.*
.terraform/
.terraform.lock.hcl

# Test results
test-results/
.pytest_cache/
.coverage
htmlcov/

# Node modules (if using Node.js frontend)
node_modules/
npm-debug.log*
EOF

# Create backend application
echo "üîß Creating backend application..."
cat > backend/main.py << EOF
"""
${PROJECT_NAME} - FastAPI Backend Application
Generated by AI Agents Framework
"""

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import os
from typing import Dict, Any

# Configuration
class Settings:
    def __init__(self):
        self.env = os.getenv("${PROJECT_NAME^^}_ENV", "development")
        self.debug = os.getenv("${PROJECT_NAME^^}_DEBUG", "false").lower() == "true"
        self.openai_api_key = os.getenv("${PROJECT_NAME^^}_OPENAI_API_KEY", "")
        self.cors_origins = os.getenv("${PROJECT_NAME^^}_CORS_ORIGINS", "").split(",")

settings = Settings()

# FastAPI application
app = FastAPI(
    title="${PROJECT_NAME} API",
    description="AI Agents Backend API",
    version="1.0.0",
    debug=settings.debug
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.cors_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/")
async def root():
    return {"message": "Welcome to ${PROJECT_NAME} AI Agents API"}

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "environment": settings.env,
        "service": "${PROJECT_NAME}-backend"
    }

@app.get("/api/v1/agents")
async def list_agents():
    """List available agents"""
    return {
        "agents": [
            {
                "name": "example_agent",
                "type": "simple",
                "status": "available",
                "description": "Example agent for demonstration"
            }
        ]
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
EOF

# Create backend requirements
cat > backend/requirements.txt << 'EOF'
fastapi==0.104.1
uvicorn[standard]==0.24.0
pydantic==2.5.0
pydantic-settings==2.1.0
sqlalchemy==2.0.23
alembic==1.13.1
psycopg2-binary==2.9.9
redis==5.0.1
openai==1.3.7
anthropic==0.7.7
structlog==23.2.0
python-multipart==0.0.6
python-jose[cryptography]==3.3.0
passlib[bcrypt]==1.7.4
aiofiles==23.2.1
httpx==0.25.2
python-dotenv==1.0.0
pytest==7.4.3
pytest-asyncio==0.21.1
pytest-cov==4.1.0
EOF

# Create backend Dockerfile
cat > backend/Dockerfile << 'EOF'
FROM python:3.11-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    gcc \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements and install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir --upgrade pip
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY . .

# Create directories
RUN mkdir -p /app/data /app/logs

# Expose port
EXPOSE 8000

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# Run application
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
EOF

# Create frontend application
echo "üé® Creating frontend application..."
cat > frontend/main.py << EOF
"""
${PROJECT_NAME} - Streamlit Frontend Application
Generated by AI Agents Framework
"""

import streamlit as st
import requests
import os
from typing import Dict, Any

# Configuration
BACKEND_HOST = os.getenv("${PROJECT_NAME^^}_BACKEND_HOST", "localhost")
BACKEND_PORT = os.getenv("${PROJECT_NAME^^}_BACKEND_PORT", "8000")
BACKEND_URL = f"http://{BACKEND_HOST}:{BACKEND_PORT}"

# Page configuration
st.set_page_config(
    page_title="${PROJECT_NAME} AI Agents",
    page_icon="ü§ñ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Title
st.title("ü§ñ ${PROJECT_NAME} AI Agents")
st.markdown("Welcome to your AI Agents system!")

# Sidebar
with st.sidebar:
    st.header("Navigation")
    page = st.selectbox("Choose a page", ["Home", "Agents", "Health"])

# Main content
if page == "Home":
    st.header("üè† Home")
    st.markdown("""
    This is your AI Agents system built with the framework. Here you can:
    
    - ü§ñ Manage and interact with AI agents
    - üìä Monitor system health
    - üîß Configure agent workflows
    - üìà View analytics and insights
    """)
    
    # Quick stats
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("Active Agents", "1", "0")
    
    with col2:
        st.metric("Total Requests", "0", "0")
    
    with col3:
        st.metric("Success Rate", "100%", "0%")

elif page == "Agents":
    st.header("ü§ñ Available Agents")
    
    try:
        response = requests.get(f"{BACKEND_URL}/api/v1/agents")
        if response.status_code == 200:
            agents = response.json()["agents"]
            
            for agent in agents:
                with st.expander(f"ü§ñ {agent['name']}"):
                    st.write(f"**Type:** {agent['type']}")
                    st.write(f"**Status:** {agent['status']}")
                    st.write(f"**Description:** {agent['description']}")
                    
                    if st.button(f"Interact with {agent['name']}", key=agent['name']):
                        st.success(f"Interacting with {agent['name']}...")
        else:
            st.error("Could not load agents from backend")
            
    except requests.exceptions.ConnectionError:
        st.error(f"Could not connect to backend at {BACKEND_URL}")
        st.info("Make sure the backend service is running")

elif page == "Health":
    st.header("üè• System Health")
    
    try:
        response = requests.get(f"{BACKEND_URL}/health")
        if response.status_code == 200:
            health = response.json()
            
            st.success("‚úÖ Backend is healthy")
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.metric("Status", health["status"])
                st.metric("Environment", health["environment"])
            
            with col2:
                st.metric("Service", health["service"])
            
        else:
            st.error("‚ùå Backend health check failed")
            
    except requests.exceptions.ConnectionError:
        st.error(f"‚ùå Could not connect to backend at {BACKEND_URL}")

# Footer
st.markdown("---")
st.markdown("Built with the AI Agents Framework üöÄ")
EOF

# Create frontend requirements
cat > frontend/requirements.txt << 'EOF'
streamlit==1.28.1
requests==2.31.0
pandas==2.1.4
plotly==5.17.0
python-dotenv==1.0.0
httpx==0.25.2
EOF

# Create frontend Dockerfile
cat > frontend/Dockerfile << 'EOF'
FROM python:3.11-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements and install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir --upgrade pip
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY . .

# Expose port
EXPOSE 8501

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8501/_stcore/health || exit 1

# Run Streamlit
CMD ["streamlit", "run", "main.py", "--server.port=8501", "--server.address=0.0.0.0", "--server.headless=true"]
EOF

# Create docker-compose for local development
echo "üê≥ Creating Docker Compose configuration..."
cat > docker-compose.yml << EOF
version: '3.8'

services:
  backend:
    build: ./backend
    container_name: ${PROJECT_NAME}-backend
    ports:
      - "\${${PROJECT_NAME^^}_BACKEND_PORT:-8000}:8000"
    environment:
      - ${PROJECT_NAME^^}_ENV=development
      - ${PROJECT_NAME^^}_DATABASE_HOST=database
      - ${PROJECT_NAME^^}_REDIS_URL=redis://redis:6379/0
    env_file:
      - .env.local
    volumes:
      - ./backend:/app
      - ./data:/app/data
      - ./logs:/app/logs
    depends_on:
      - database
      - redis
    networks:
      - ${PROJECT_NAME}-network
    restart: unless-stopped

  frontend:
    build: ./frontend
    container_name: ${PROJECT_NAME}-frontend
    ports:
      - "\${${PROJECT_NAME^^}_FRONTEND_PORT:-8501}:8501"
    environment:
      - ${PROJECT_NAME^^}_BACKEND_HOST=backend
      - ${PROJECT_NAME^^}_BACKEND_PORT=8000
    env_file:
      - .env.local
    volumes:
      - ./frontend:/app
    depends_on:
      - backend
    networks:
      - ${PROJECT_NAME}-network
    restart: unless-stopped

  database:
    image: postgres:15-alpine
    container_name: ${PROJECT_NAME}-database
    environment:
      - POSTGRES_DB=\${${PROJECT_NAME^^}_DATABASE_NAME:-${PROJECT_NAME}_dev}
      - POSTGRES_USER=\${${PROJECT_NAME^^}_DATABASE_USER:-${PROJECT_NAME}_user}
      - POSTGRES_PASSWORD=\${${PROJECT_NAME^^}_DATABASE_PASSWORD:-dev_password}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - ${PROJECT_NAME}-network
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    container_name: ${PROJECT_NAME}-redis
    volumes:
      - redis_data:/data
    networks:
      - ${PROJECT_NAME}-network
    restart: unless-stopped

volumes:
  postgres_data:
  redis_data:

networks:
  ${PROJECT_NAME}-network:
    driver: bridge
EOF

# Create development scripts
echo "üìù Creating development scripts..."
mkdir -p scripts

# Start script
cat > scripts/start.sh << 'EOF'
#!/bin/bash
# Start local development environment

set -e

echo "üöÄ Starting local development environment..."

# Check environment file
if [ ! -f ".env.local" ]; then
    echo "üìù Creating .env.local from template..."
    cp .env.template .env.local
    echo "‚ö†Ô∏è  Please edit .env.local with your settings"
fi

# Create directories
mkdir -p data logs

# Start services
docker-compose up -d

# Wait for services
echo "‚è≥ Waiting for services to start..."
sleep 10

# Check health
echo "üîç Checking service health..."
docker-compose ps

echo "‚úÖ Development environment ready!"
echo ""
echo "üåê Frontend: http://localhost:8501"
echo "üåê Backend: http://localhost:8000"
echo "üåê API Docs: http://localhost:8000/docs"
EOF

# Stop script
cat > scripts/stop.sh << 'EOF'
#!/bin/bash
# Stop local development environment

echo "üõë Stopping development environment..."
docker-compose down
echo "‚úÖ Environment stopped"
EOF

# Test script
cat > scripts/test.sh << 'EOF'
#!/bin/bash
# Run comprehensive test suite

set -e

echo "üß™ Running test suite..."

# Start test environment
docker-compose -f docker-compose.test.yml up -d || docker-compose up -d

# Wait for services
sleep 10

# Run tests (when test files exist)
echo "üî¨ Running tests..."
if [ -d "tests" ]; then
    docker-compose exec backend python -m pytest tests/ -v || echo "‚ö†Ô∏è No tests found yet"
else
    echo "‚ö†Ô∏è No tests directory found"
fi

# Stop test environment
docker-compose down

echo "‚úÖ Tests completed"
EOF

# Make scripts executable
chmod +x scripts/*.sh

# Create README
echo "üìö Creating README..."
cat > README.md << EOF
# ${PROJECT_NAME}

AI Agents system built with the AI Agents Framework.

## Quick Start

1. **Setup**: The project has been initialized with all necessary files
2. **Configure**: Edit \`.env.local\` with your settings (OpenAI API key is already set)
3. **Start**: Run \`./scripts/start.sh\` to start the development environment
4. **Develop**: Access the frontend at http://localhost:8501

## Commands

\`\`\`bash
# Start development environment
./scripts/start.sh

# Stop development environment
./scripts/stop.sh

# Run tests
./scripts/test.sh
\`\`\`

## Structure

- \`backend/\` - FastAPI backend with AI agents
- \`frontend/\` - Streamlit web interface
- \`database/\` - Database schemas and migrations
- \`infrastructure/\` - Terraform for cloud deployment
- \`tests/\` - Comprehensive test suite

## Next Steps

1. Add your AI agents to \`backend/core/agents/\`
2. Customize the frontend in \`frontend/\`
3. Set up cloud deployment with the infrastructure scripts
4. Add comprehensive tests in \`tests/\`

## Framework Documentation

This project follows the AI Agents Framework patterns. See the framework documentation for detailed guides on implementing different complexity levels.
EOF

# Initialize git repository
echo "üìù Initializing git repository..."
git init
git add .
git commit -m "Initial commit: ${PROJECT_NAME} AI Agents project

Generated by AI Agents Framework setup script.
Includes:
- FastAPI backend with health endpoints
- Streamlit frontend with basic UI
- Docker Compose for local development
- Environment configuration with ${PROJECT_NAME^^}_ prefixes
- Development scripts for easy management"

echo ""
echo "üéâ Project setup completed successfully!"
echo ""
echo "üìÅ Project created in: $(pwd)"
echo "‚öôÔ∏è Configuration file: .env.local (OpenAI API key already set)"
echo ""
echo "üöÄ Next steps:"
echo "1. Edit .env.local if you need to change any settings"
echo "2. Run: ./scripts/start.sh"
echo "3. Visit: http://localhost:8501"
echo ""
echo "üìö See README.md for more information"
EOF

chmod +x scripts/setup-project.sh
```

### Cloud Deployment Setup
```bash
#!/bin/bash
# scripts/setup-cloud-deployment.sh - Google Cloud deployment setup

set -e

PROJECT_NAME=${1}
GCP_PROJECT_ID=${2}
OPENAI_API_KEY=${3}

if [ -z "$PROJECT_NAME" ] || [ -z "$GCP_PROJECT_ID" ] || [ -z "$OPENAI_API_KEY" ]; then
    echo "‚ùå Usage: $0 <project-name> <gcp-project-id> <openai-api-key>"
    echo "   Example: $0 my-ai-agents my-gcp-project sk-..."
    exit 1
fi

echo "‚òÅÔ∏è Setting up Google Cloud deployment for $PROJECT_NAME"

# Check if gcloud is installed and authenticated
if ! command -v gcloud &> /dev/null; then
    echo "‚ùå gcloud CLI not found. Please install Google Cloud SDK"
    exit 1
fi

if ! gcloud auth list --filter=status:ACTIVE --format="value(account)" | head -n1 > /dev/null; then
    echo "‚ùå Not authenticated with Google Cloud. Run: gcloud auth login"
    exit 1
fi

# Set project
echo "üîß Setting up Google Cloud project..."
gcloud config set project $GCP_PROJECT_ID

# Enable required APIs
echo "üîå Enabling required APIs..."
gcloud services enable run.googleapis.com
gcloud services enable sql-component.googleapis.com
gcloud services enable sqladmin.googleapis.com
gcloud services enable redis.googleapis.com
gcloud services enable secretmanager.googleapis.com
gcloud services enable cloudbuild.googleapis.com
gcloud services enable artifactregistry.googleapis.com
gcloud services enable monitoring.googleapis.com

# Create Artifact Registry repository
echo "üì¶ Creating Artifact Registry repository..."
gcloud artifacts repositories create ${PROJECT_NAME}-repo \
    --repository-format=docker \
    --location=us-central1 \
    --description="Docker repository for ${PROJECT_NAME}"

# Create Secret Manager secrets
echo "üîê Creating secrets in Secret Manager..."

# OpenAI API Key
echo $OPENAI_API_KEY | gcloud secrets create ${PROJECT_NAME}-openai-api-key --data-file=-

# Generate JWT secret
JWT_SECRET=$(openssl rand -base64 32)
echo $JWT_SECRET | gcloud secrets create ${PROJECT_NAME}-jwt-secret --data-file=-

# Generate app secret key
APP_SECRET=$(openssl rand -base64 32)
echo $APP_SECRET | gcloud secrets create ${PROJECT_NAME}-app-secret-key --data-file=-

# Create service account for Cloud Build
echo "üë§ Creating service account..."
gcloud iam service-accounts create ${PROJECT_NAME}-cloudbuild \
    --display-name="${PROJECT_NAME} Cloud Build Service Account"

# Grant necessary permissions
echo "üîë Granting permissions..."
gcloud projects add-iam-policy-binding $GCP_PROJECT_ID \
    --member="serviceAccount:${PROJECT_NAME}-cloudbuild@${GCP_PROJECT_ID}.iam.gserviceaccount.com" \
    --role="roles/run.admin"

gcloud projects add-iam-policy-binding $GCP_PROJECT_ID \
    --member="serviceAccount:${PROJECT_NAME}-cloudbuild@${GCP_PROJECT_ID}.iam.gserviceaccount.com" \
    --role="roles/iam.serviceAccountUser"

gcloud projects add-iam-policy-binding $GCP_PROJECT_ID \
    --member="serviceAccount:${PROJECT_NAME}-cloudbuild@${GCP_PROJECT_ID}.iam.gserviceaccount.com" \
    --role="roles/secretmanager.secretAccessor"

# Create Cloud Build trigger
echo "üîÑ Creating Cloud Build trigger..."
cat > cloudbuild-trigger.yaml << EOF
name: ${PROJECT_NAME}-deploy
description: "Deploy ${PROJECT_NAME} to Cloud Run"
github:
  owner: YOUR_GITHUB_USERNAME
  name: ${PROJECT_NAME}
  push:
    branch: ^main$
filename: cloudbuild.yaml
substitutions:
  _PROJECT_NAME: ${PROJECT_NAME}
  _REPOSITORY: ${PROJECT_NAME}-repo
  _REGION: us-central1
EOF

gcloud builds triggers create github \
    --trigger-config=cloudbuild-trigger.yaml

# Create Terraform state bucket
echo "ü™£ Creating Terraform state bucket..."
gsutil mb gs://${PROJECT_NAME}-terraform-state-${GCP_PROJECT_ID} || echo "Bucket may already exist"
gsutil versioning set on gs://${PROJECT_NAME}-terraform-state-${GCP_PROJECT_ID}

# Update project configuration
echo "‚öôÔ∏è Updating project configuration..."
if [ -f ".env.local" ]; then
    # Update GCP project ID in environment file
    sed -i.bak "s/your-gcp-project-id/$GCP_PROJECT_ID/" .env.local
    rm .env.local.bak
fi

# Create infrastructure configuration
echo "üèóÔ∏è Creating infrastructure configuration..."
mkdir -p infrastructure/terraform/environments/{dev,staging,prod}

# Create dev environment config
cat > infrastructure/terraform/environments/dev/terraform.tfvars << EOF
project_id   = "${GCP_PROJECT_ID}-dev"
project_name = "${PROJECT_NAME}"
environment  = "dev"
region       = "us-central1"

# Development settings
database_tier        = "db-f1-micro"
redis_memory_size_gb = 1
min_instances        = 0
max_instances        = 3
EOF

# Create staging environment config
cat > infrastructure/terraform/environments/staging/terraform.tfvars << EOF
project_id   = "${GCP_PROJECT_ID}-staging"
project_name = "${PROJECT_NAME}"
environment  = "staging"
region       = "us-central1"

# Staging settings
database_tier        = "db-g1-small"
redis_memory_size_gb = 2
min_instances        = 1
max_instances        = 5
EOF

# Create production environment config
cat > infrastructure/terraform/environments/prod/terraform.tfvars << EOF
project_id   = "${GCP_PROJECT_ID}-prod"
project_name = "${PROJECT_NAME}"
environment  = "prod"
region       = "us-central1"

# Production settings
database_tier        = "db-standard-2"
redis_memory_size_gb = 4
min_instances        = 2
max_instances        = 20
EOF

echo ""
echo "‚úÖ Google Cloud deployment setup completed!"
echo ""
echo "üìã Summary:"
echo "   Project: $GCP_PROJECT_ID"
echo "   Artifact Registry: ${PROJECT_NAME}-repo"
echo "   Secrets: Created in Secret Manager"
echo "   Service Account: ${PROJECT_NAME}-cloudbuild"
echo ""
echo "üöÄ Next steps:"
echo "1. Update cloudbuild-trigger.yaml with your GitHub username"
echo "2. Push your code to GitHub to trigger the first deployment"
echo "3. Configure additional environments (staging, prod) as needed"
echo ""
echo "üìö See infrastructure/terraform/ for Terraform configurations"
```

### Testing Automation
```bash
#!/bin/bash
# scripts/run-comprehensive-tests.sh - Complete test automation

set -e

echo "üß™ Running comprehensive test suite for cloud deployment..."

# Configuration
PROJECT_NAME=${1:-$(basename $(pwd))}
ENVIRONMENT=${2:-test}

# Set test environment variables
export ${PROJECT_NAME^^}_ENV=$ENVIRONMENT
export ${PROJECT_NAME^^}_LOG_LEVEL=DEBUG
export ${PROJECT_NAME^^}_DATABASE_URL="postgresql://test_user:test_pass@localhost:5432/test_db"
export ${PROJECT_NAME^^}_REDIS_URL="redis://localhost:6379/1"

# Create test results directory
mkdir -p test-results

echo "üê≥ Starting test environment..."
docker-compose -f docker-compose.test.yml up -d 2>/dev/null || docker-compose up -d

# Wait for services with timeout
echo "‚è≥ Waiting for services to be ready..."
timeout=60
counter=0

while [ $counter -lt $timeout ]; do
    if docker-compose exec -T backend curl -f http://localhost:8000/health > /dev/null 2>&1; then
        echo "‚úÖ Backend is ready"
        break
    fi
    sleep 2
    counter=$((counter + 2))
done

if [ $counter -ge $timeout ]; then
    echo "‚ùå Backend failed to start within $timeout seconds"
    docker-compose logs backend
    exit 1
fi

# Test categories
test_categories=("unit" "integration" "e2e" "performance")
test_results=()

for category in "${test_categories[@]}"; do
    if [ -d "tests/$category" ] && [ "$(ls -A tests/$category)" ]; then
        echo "üî¨ Running $category tests..."
        
        if docker-compose exec -T backend python -m pytest tests/$category/ \
            -v \
            --junitxml=/app/test-results/${category}-tests.xml \
            --cov=core \
            --cov=api \
            --cov-report=xml:/app/test-results/${category}-coverage.xml; then
            echo "‚úÖ $category tests passed"
            test_results+=("$category:PASS")
        else
            echo "‚ùå $category tests failed"
            test_results+=("$category:FAIL")
        fi
    else
        echo "‚ö†Ô∏è No $category tests found, skipping..."
        test_results+=("$category:SKIP")
    fi
done

# Security tests
if command -v safety &> /dev/null; then
    echo "üîí Running security tests..."
    if docker-compose exec -T backend safety check --json > test-results/security-scan.json; then
        echo "‚úÖ Security scan passed"
        test_results+=("security:PASS")
    else
        echo "‚ö†Ô∏è Security issues found, check test-results/security-scan.json"
        test_results+=("security:WARN")
    fi
fi

# Code quality checks
if command -v flake8 &> /dev/null; then
    echo "üìù Running code quality checks..."
    if docker-compose exec -T backend flake8 . --output-file=/app/test-results/flake8-report.txt; then
        echo "‚úÖ Code quality checks passed"
        test_results+=("quality:PASS")
    else
        echo "‚ö†Ô∏è Code quality issues found"
        test_results+=("quality:WARN")
    fi
fi

# Performance benchmarks
echo "‚ö° Running performance benchmarks..."
docker-compose exec -T backend python -c "
import time
import requests
import statistics

# Simple performance test
times = []
for i in range(10):
    start = time.time()
    response = requests.get('http://localhost:8000/health')
    end = time.time()
    if response.status_code == 200:
        times.append(end - start)

if times:
    avg_time = statistics.mean(times)
    print(f'Average response time: {avg_time:.3f}s')
    
    with open('/app/test-results/performance-benchmark.txt', 'w') as f:
        f.write(f'Average response time: {avg_time:.3f}s\\n')
        f.write(f'Min response time: {min(times):.3f}s\\n')
        f.write(f'Max response time: {max(times):.3f}s\\n')
        f.write(f'Total requests: {len(times)}\\n')
" && echo "‚úÖ Performance benchmarks completed"

# Cleanup
echo "üßπ Cleaning up test environment..."
docker-compose down > /dev/null 2>&1

# Generate test summary
echo "üìä Generating test summary..."
cat > test-results/summary.json << EOF
{
    "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
    "project": "$PROJECT_NAME",
    "environment": "$ENVIRONMENT",
    "test_results": [
EOF

first=true
for result in "${test_results[@]}"; do
    category=$(echo $result | cut -d: -f1)
    status=$(echo $result | cut -d: -f2)
    
    if [ "$first" = true ]; then
        first=false
    else
        echo "," >> test-results/summary.json
    fi
    
    echo "        {\"category\": \"$category\", \"status\": \"$status\"}" >> test-results/summary.json
done

cat >> test-results/summary.json << EOF
    ],
    "overall_status": "$(if [[ "${test_results[*]}" =~ "FAIL" ]]; then echo "FAILED"; else echo "PASSED"; fi)",
    "cloud_ready": $(if [[ "${test_results[*]}" =~ "FAIL" ]]; then echo "false"; else echo "true"; fi)
}
EOF

# Create deployment evidence
cp test-results/summary.json test-results.json

# Display results
echo ""
echo "üìã Test Results Summary:"
echo "========================"
for result in "${test_results[@]}"; do
    category=$(echo $result | cut -d: -f1)
    status=$(echo $result | cut -d: -f2)
    
    case $status in
        "PASS") icon="‚úÖ" ;;
        "FAIL") icon="‚ùå" ;;
        "WARN") icon="‚ö†Ô∏è" ;;
        "SKIP") icon="‚è≠Ô∏è" ;;
    esac
    
    echo "$icon $category: $status"
done

echo ""
if [[ "${test_results[*]}" =~ "FAIL" ]]; then
    echo "‚ùå Some tests failed. Fix issues before cloud deployment."
    exit 1
else
    echo "‚úÖ All tests passed! Ready for cloud deployment."
    echo "üì§ Push your changes to trigger the CI/CD pipeline."
fi
```

### Monitoring Setup
```bash
#!/bin/bash
# scripts/setup-monitoring.sh - Set up comprehensive monitoring

set -e

PROJECT_NAME=${1}
GCP_PROJECT_ID=${2}

if [ -z "$PROJECT_NAME" ] || [ -z "$GCP_PROJECT_ID" ]; then
    echo "‚ùå Usage: $0 <project-name> <gcp-project-id>"
    exit 1
fi

echo "üìä Setting up monitoring for $PROJECT_NAME"

# Create alerting policies
echo "üö® Creating alerting policies..."

# High error rate alert
gcloud alpha monitoring policies create --policy-from-file=- << EOF
displayName: "${PROJECT_NAME} High Error Rate"
combiner: OR
conditions:
  - displayName: "Cloud Run Error Rate"
    conditionThreshold:
      filter: 'resource.type="cloud_run_revision" AND resource.label.service_name=~"${PROJECT_NAME}-.*"'
      comparison: COMPARISON_GT
      thresholdValue: 0.05
      duration: 300s
      aggregations:
        - alignmentPeriod: 60s
          perSeriesAligner: ALIGN_RATE
notificationChannels: []
alertStrategy:
  autoClose: 1800s
EOF

# High latency alert
gcloud alpha monitoring policies create --policy-from-file=- << EOF
displayName: "${PROJECT_NAME} High Latency"
combiner: OR
conditions:
  - displayName: "Cloud Run High Latency"
    conditionThreshold:
      filter: 'resource.type="cloud_run_revision" AND resource.label.service_name=~"${PROJECT_NAME}-.*"'
      comparison: COMPARISON_GT
      thresholdValue: 5000
      duration: 300s
      aggregations:
        - alignmentPeriod: 60s
          perSeriesAligner: ALIGN_MEAN
notificationChannels: []
alertStrategy:
  autoClose: 1800s
EOF

# Create custom dashboard
echo "üìà Creating monitoring dashboard..."
cat > monitoring-dashboard.json << EOF
{
  "displayName": "${PROJECT_NAME} System Dashboard",
  "gridLayout": {
    "widgets": [
      {
        "title": "Request Count",
        "xyChart": {
          "dataSets": [
            {
              "timeSeriesQuery": {
                "timeSeriesFilter": {
                  "filter": "resource.type=\"cloud_run_revision\" AND resource.label.service_name=~\"${PROJECT_NAME}-.*\"",
                  "aggregation": {
                    "alignmentPeriod": "60s",
                    "perSeriesAligner": "ALIGN_RATE"
                  }
                }
              },
              "plotType": "LINE"
            }
          ]
        }
      },
      {
        "title": "Response Latency",
        "xyChart": {
          "dataSets": [
            {
              "timeSeriesQuery": {
                "timeSeriesFilter": {
                  "filter": "resource.type=\"cloud_run_revision\" AND resource.label.service_name=~\"${PROJECT_NAME}-.*\"",
                  "aggregation": {
                    "alignmentPeriod": "60s",
                    "perSeriesAligner": "ALIGN_MEAN"
                  }
                }
              },
              "plotType": "LINE"
            }
          ]
        }
      }
    ]
  }
}
EOF

gcloud monitoring dashboards create --config-from-file=monitoring-dashboard.json

# Create log-based metrics
echo "üìù Creating log-based metrics..."
gcloud logging metrics create ${PROJECT_NAME}_error_count \
    --description="Count of error log entries" \
    --log-filter="resource.type=\"cloud_run_revision\" AND resource.labels.service_name=~\"${PROJECT_NAME}-.*\" AND severity>=ERROR"

# Set up log exports (optional)
echo "üì§ Setting up log exports..."
gcloud logging sinks create ${PROJECT_NAME}_error_logs \
    bigquery.googleapis.com/projects/${GCP_PROJECT_ID}/datasets/logs \
    --log-filter="resource.type=\"cloud_run_revision\" AND resource.labels.service_name=~\"${PROJECT_NAME}-.*\" AND severity>=ERROR"

echo "‚úÖ Monitoring setup completed!"
echo ""
echo "üìä Dashboard: Check Google Cloud Console > Monitoring > Dashboards"
echo "üö® Alerts: Check Google Cloud Console > Monitoring > Alerting"
echo "üìù Logs: Check Google Cloud Console > Logging"
```

### Backup and Disaster Recovery
```bash
#!/bin/bash
# scripts/setup-backup.sh - Set up backup and disaster recovery

set -e

PROJECT_NAME=${1}
GCP_PROJECT_ID=${2}

if [ -z "$PROJECT_NAME" ] || [ -z "$GCP_PROJECT_ID" ]; then
    echo "‚ùå Usage: $0 <project-name> <gcp-project-id>"
    exit 1
fi

echo "üíæ Setting up backup and disaster recovery for $PROJECT_NAME"

# Create backup bucket
echo "ü™£ Creating backup bucket..."
gsutil mb gs://${PROJECT_NAME}-backups-${GCP_PROJECT_ID} || echo "Bucket may already exist"
gsutil lifecycle set /dev/stdin gs://${PROJECT_NAME}-backups-${GCP_PROJECT_ID} << EOF
{
  "lifecycle": {
    "rule": [
      {
        "action": {"type": "Delete"},
        "condition": {"age": 90}
      }
    ]
  }
}
EOF

# Database backup script
cat > scripts/backup-database.sh << 'EOF'
#!/bin/bash
# Backup Cloud SQL database

set -e

PROJECT_ID=${1}
INSTANCE_NAME=${2}
DATABASE_NAME=${3}

if [ -z "$PROJECT_ID" ] || [ -z "$INSTANCE_NAME" ] || [ -z "$DATABASE_NAME" ]; then
    echo "‚ùå Usage: $0 <project-id> <instance-name> <database-name>"
    exit 1
fi

BACKUP_ID="${DATABASE_NAME}-backup-$(date +%Y%m%d-%H%M%S)"

echo "üíæ Creating database backup: $BACKUP_ID"

gcloud sql backups create \
    --instance=$INSTANCE_NAME \
    --project=$PROJECT_ID \
    --description="Automated backup of $DATABASE_NAME"

echo "‚úÖ Database backup created: $BACKUP_ID"
EOF

# Application data backup script
cat > scripts/backup-data.sh << 'EOF'
#!/bin/bash
# Backup application data

set -e

PROJECT_NAME=${1}
ENVIRONMENT=${2:-prod}

BACKUP_DATE=$(date +%Y%m%d-%H%M%S)
BACKUP_FILE="${PROJECT_NAME}-${ENVIRONMENT}-data-${BACKUP_DATE}.tar.gz"

echo "üíæ Creating application data backup..."

# Backup configuration
mkdir -p backup-temp/config
cp .env.production backup-temp/config/ 2>/dev/null || echo "No production config found"

# Backup logs (recent)
mkdir -p backup-temp/logs
find logs/ -name "*.log" -mtime -7 -exec cp {} backup-temp/logs/ \; 2>/dev/null || echo "No recent logs found"

# Backup data directory
if [ -d "data" ]; then
    cp -r data backup-temp/
fi

# Create archive
tar -czf $BACKUP_FILE -C backup-temp .

# Upload to Cloud Storage
gsutil cp $BACKUP_FILE gs://${PROJECT_NAME}-backups-*/application-data/

# Cleanup
rm -rf backup-temp $BACKUP_FILE

echo "‚úÖ Application data backup completed: $BACKUP_FILE"
EOF

# Disaster recovery script
cat > scripts/disaster-recovery.sh << 'EOF'
#!/bin/bash
# Disaster recovery procedures

set -e

PROJECT_NAME=${1}
ENVIRONMENT=${2:-prod}
BACKUP_DATE=${3}

if [ -z "$PROJECT_NAME" ] || [ -z "$BACKUP_DATE" ]; then
    echo "‚ùå Usage: $0 <project-name> [environment] <backup-date>"
    echo "   Example: $0 myproject prod 20231215-143000"
    exit 1
fi

echo "üö® Starting disaster recovery for $PROJECT_NAME ($ENVIRONMENT)"
echo "‚ö†Ô∏è  This will restore from backup dated: $BACKUP_DATE"

read -p "Are you sure you want to proceed? (yes/no): " confirm
if [ "$confirm" != "yes" ]; then
    echo "‚ùå Disaster recovery cancelled"
    exit 1
fi

# Stop current services
echo "üõë Stopping current services..."
gcloud run services update ${PROJECT_NAME}-${ENVIRONMENT}-backend \
    --min-instances=0 \
    --max-instances=0 \
    --region=us-central1

# Restore database
echo "üíæ Restoring database..."
# Add database restoration commands here

# Restore application data
echo "üìÅ Restoring application data..."
BACKUP_FILE="${PROJECT_NAME}-${ENVIRONMENT}-data-${BACKUP_DATE}.tar.gz"
gsutil cp gs://${PROJECT_NAME}-backups-*/application-data/$BACKUP_FILE .
tar -xzf $BACKUP_FILE

# Restart services
echo "üöÄ Restarting services..."
gcloud run services update ${PROJECT_NAME}-${ENVIRONMENT}-backend \
    --min-instances=1 \
    --max-instances=10 \
    --region=us-central1

# Health check
echo "üè• Running health checks..."
sleep 30
BACKEND_URL=$(gcloud run services describe ${PROJECT_NAME}-${ENVIRONMENT}-backend \
    --region=us-central1 \
    --format='value(status.url)')

if curl -f "$BACKEND_URL/health" > /dev/null 2>&1; then
    echo "‚úÖ Disaster recovery completed successfully!"
else
    echo "‚ùå Health check failed after recovery"
    exit 1
fi

# Cleanup
rm -f $BACKUP_FILE

echo "‚úÖ Disaster recovery procedures completed"
EOF

# Make scripts executable
chmod +x scripts/backup-*.sh scripts/disaster-recovery.sh

# Create cron job for automated backups
cat > scripts/setup-automated-backups.sh << 'EOF'
#!/bin/bash
# Set up automated backups

PROJECT_NAME=${1}
GCP_PROJECT_ID=${2}

# Create Cloud Scheduler job for database backups
gcloud scheduler jobs create http ${PROJECT_NAME}-database-backup \
    --schedule="0 2 * * *" \
    --uri="https://sqladmin.googleapis.com/sql/v1beta4/projects/${GCP_PROJECT_ID}/instances/${PROJECT_NAME}-prod-db/backupRuns" \
    --http-method=POST \
    --headers="Authorization=Bearer $(gcloud auth application-default print-access-token)" \
    --time-zone="UTC"

# Create Cloud Function for application data backup
# (Cloud Function code would be more complex, this is simplified)

echo "‚úÖ Automated backup jobs created"
echo "üìÖ Database backups: Daily at 2:00 AM UTC"
echo "üìÖ Application data: Configure Cloud Function as needed"
EOF

chmod +x scripts/setup-automated-backups.sh

echo "‚úÖ Backup and disaster recovery setup completed!"
echo ""
echo "üìù Available scripts:"
echo "   ./scripts/backup-database.sh - Manual database backup"
echo "   ./scripts/backup-data.sh - Manual application data backup"
echo "   ./scripts/disaster-recovery.sh - Disaster recovery procedures"
echo "   ./scripts/setup-automated-backups.sh - Set up automated backups"
```

---

## Next Steps

- **Framework Integration**: [Integrating with Framework Levels](framework_integration.md)
- **Production Checklist**: [Production Deployment Guide](production_checklist.md)
- **Monitoring**: [Observability and Alerting](observability.md)

---

*These scripts provide complete automation for the entire development and deployment lifecycle, from initial setup to production monitoring and disaster recovery.*